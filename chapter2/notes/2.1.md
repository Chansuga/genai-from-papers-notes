# 2.1 埋め込みと分布仮説

- 機械学習におけるテキストデータの特徴量化の単純な方法として**局所表現**と呼ばれるワンホットベクトルが考えられる
- 局所表現とは語彙 $\nu$ (単語の集まり)に対して $|\nu|$ 次元ベクトルを準備し、該当する要素が1で他が0のベクトルで表現する方法である

    例） $\nu = \{ 物理学, 数学, 人文学, 社会学 \}$ の場合

```math
l_{物理学}=\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} , \quad
l_{数学}=\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} , \quad
l_{人文学}=\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} , \quad
l_{社会学}=\begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}
```

- 局所表現はベクトルとなるため、線形変換や関数の適用をすることで
  機械学習モデルで扱うことが可能になる
- しかし、局所表現では以下のような問題がある
    - トークンが増えると次元が増大し、計算量が増える
    例）$n$個のトークンを線形変換する場合は $\mathcal{O}(|\nu|^2N)$ の計算量となる
    - トークンの類似性や関係性などの言語的な情報を全く反映しないため特徴量として有益なところがない
    例） 任意の2つのトークンのEuclid距離は常に $\sqrt{2}$ である
- 局所表現の問題を解決するための方向性として**分散表現**がある。これは各次元に連続的に分布したベクトルとして表現するものである。

    例）トークンの情報を表すのに必要な次元が $\mathbb{R}^{log_2{|\nu|}}$ であると仮定した場合、以下のような分散表現が考えられる

```math
d_{物理学}=\begin{pmatrix} 0.7 \\ 0.3  \end{pmatrix} , \quad
d_{数学}=\begin{pmatrix} 0.8 \\ 0.2  \end{pmatrix} , \quad
d_{人文学}=\begin{pmatrix} 0.1 \\ 0.9  \end{pmatrix} , \quad
d_{社会学}=\begin{pmatrix} -0.1 \\ 0.8  \end{pmatrix}
```

- 分散表現は局所表現と異なり、小さい次元でトークンの情報を扱え、トークン間の類似性や関係性などを表現しやすくなる
- トークンを文産業減として扱える特徴量に変換する操作を**埋め込み**と呼ぶ
- 埋め込みは機械学習では必要不可欠であり、解きたいタスクに適した埋め込み方法を獲得することが重要である
- 埋め込みの獲得を目指すうえで、できるだけ人間による外部知識を使わずにデータから学習することが都合が良い
- 埋め込みのデータによる学習手法としては、手持ちのテキストから問題を作ってそれを解くことで、言語特性を学習する方法が広く用いられる
    1. トークンの穴埋め問題
    2. 次に来るトークン予測
    3. ある文章と次の文章が連続した文章か否かの予測
- 埋め込みデータの学習は、トークンごとにランダムに初期化された分散表現を用意して、損失関数を最小化するように学習を行う
    - 例）'次に来るトークン予測'を扱う場合は以下のような損失関数 $L(\theta)$を使用したりする。この手法の前提には、**分布仮説**と呼ばれる「単語の意味はその単語が出現する文脈によって決まる」という考え方がある

```math
L(\theta) = -\sum_{i=1} \log p_{\theta}(t_i | t_{1:i-1}), \\
t_i : i番目のトークン, \\
\theta : 埋め込みのパラメータ
```

- 分布仮説の原論文としてよく引用されるのは、1965年に公開された`Contextual correlates of synonymy`というタイトルの論文である

### Contextual correlates of synonymy
示したい仮説：
「**単語のAの文脈と単語Bの文脈に共通する単語の割合は、AとBがどれだけ意味的に類似しているかの関数である**」

この論文の実験デザインとして重要なポイント
1. 注目したい単語AとBの類似度を定量的に表現すること
2. 注目したい単語を含む文章を作って文脈を定義して類似度との関係を明確にすること

まず類似度を調べる対象となるテーマ単語のペアする。
研究ではAグループとBグループを用意し、それぞれに24単語を準備する。そして、AグループとBグループから1つずつ復元抽出で単語を選び、合計で65組のペアを作った。

次に類似度の定量評価は以下の手順で実施した
- 15名と36名の大学生被験者グループに有償で参加してもらう
- 作成した65組のテーマ単語ペアを見せて、意味の類似度が高いと考える準に並べてもらう
- 類似度として $[0.0, 4.0]$ の値を付けてもらう。高いほど類似度が高く、異なるペアに対して同じ数字を付けてもよい

グループ別に各ペアの類似度の平均を計算し、それらの平均をとると $r=0.99$ となったことから、グループごとの類似度評価に違いはないとして、51人の評価をまとめて平均したものを最終的な単語AとBの類似度とする。

次に文脈を構成するためにこれらのテーマ単語を含む文章を準備する。

文章の作成は以下の手順で実施した

- 類似度評価に参加していない100名の大学生被験者グループを準備
- 50名ずつに分け、それぞれにAグループのテーマ単語とBグループのテーマ単語すべてについて2つの文章を作成してもらう
- 文章作成時の条件は、テーマ単語は名詞として使うことと文章は10語以上からなること、とする

これで48のデーマ単語それぞれに対して100個の文章を得る。

以上で必要なデータが揃ったので、文脈を具体的に定義し、文脈がどの程度同じであるかを定量的に測る方法を定義することで仮説を検証する

文脈の定義は、**対象とするテーマ単語を含む文に登場するすべての単語**とする。そして、文脈の類似度を以下で定義する

```math
M_x = \frac{\text{count}(A_x \cap B_x)}{\min(\text{count}(A_x), \text{count}(B_x))} \\
```
- $S_A$ : テーマ単語Aについて書かれた文脈（使われた単語すべての集合、重複排除なし）
- $A_x$ : $S_A$ の部分集合で、$x$ で条件づけられたもの
    - type条件 ($x=y$): $S_A$ に現れる単語を重複排除 
    - token条件 ($x=k$): 重複排除しない