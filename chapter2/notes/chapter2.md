# 第2章 入力データの特徴量化

- 機械学習モデルに対して入力を扱う際,扱いたい対象に対して,**データ化**と**特徴量化**を行う必要がある.
- 特徴量化によって、何かしらを予測したり、データを生成することが可能になる
- 各対象データのデータ化、特徴量化について
    - 画像・音声データの場合：
        - データ化と特徴量化に大きな差分はない
        - 画像の場合、例えば $R,G,B \in [0,1]$を用いて$(R,G,B)$で表現すればよい
    - テキストデータの場合：
        - データ化について
            - 物理現象のままで表現できないため、離散的な記号として扱う必要がある
            - 多様な表現かつ、計算機で扱えるバイト列にするために**UTF-8**などの文字符号化形式を用いる必要がある
        - 特徴量化について
            - 機械学習モデルは**誤差逆伝播法**を用いて学習を行うのが強力だが、自然言語は離散的な記号であるため、誤差逆伝播法を直接適用することができないという課題がある
            - 人間にとって"意味のある"特徴量にすること自体が難しい
- テキストデータの特徴量化という課題は、自然言語処理の一分野になるほど重要な課題である
- 以下の2つは特に中心的な役割を果たす
    1.  トークンから連続的な特徴量を得る**埋め込み**
    2.  テキストをトークン単位に分割する**トークン化**
- トークンとは、テキストデータを機械学習で扱う際の最小構成要素のことである
    - 状況によって指す対象は異なり、バイト列や文字、サブワードであったりする



## 2.1 埋め込みと分布仮説

- 機械学習におけるテキストデータの特徴量化の単純な方法として**局所表現**と呼ばれるワンホットベクトルが考えられる
- 局所表現とは語彙 $\nu$ (単語の集まり)に対して $|\nu|$次元ベクトルを準備し、該当する要素が1で他が0のベクトルで表現する方法である

    >例）$\nu = \{ 物理学, 数学, 人文学, 社会学 \}$ の場合
    >$$ 
    >l_{物理学}=\begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} , \quad
    >l_{数学}=\begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix} , \quad
    >l_{人文学}=\begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix} , \quad
    >l_{社会学}=\begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}
    >$$
- 局所表現はベクトルとなるため、線形変換や関数の適用をすることで
  機械学習モデルで扱うことが可能になる
- しかし、局所表現では以下のような問題がある
    - トークンが増えると次元が増大し、計算量が増える
        > 例）$n$個のトークンを線形変換する場合は $\mathcal{O}(|\nu|^2N)$ の計算量となる
    - トークンの類似性や関係性などの言語的な情報を全く反映しないため特徴量として有益なところがない
        >例） 任意の2つのトークンのEuclid距離は常に $\sqrt{2}$ である
- 局所表現の問題を解決するための方向性として**分散表現**がある。これは各次元に連続的に分布したベクトルとして表現するものである。
    >例）トークンの情報を表すのに必要な次元が  $\mathbb{R}^{log_2{|\nu|}}$であると仮定した場合、以下のような分散表現が考えられる
    >$$ 
    >d_{物理学}=\begin{pmatrix} 0.7 \\ 0.3  \end{pmatrix} , \quad
    >d_{数学}=\begin{pmatrix} 0.8 \\ 0.2  \end{pmatrix} , \quad
    >d_{人文学}=\begin{pmatrix} 0.1 \\ 0.9  \end{pmatrix} , \quad
    >d_{社会学}=\begin{pmatrix} -0.1 \\ 0.8  \end{pmatrix}
    >$$
- 分散表現は局所表現と異なり、小さい次元でトークンの情報を扱え、トークン間の類似性や関係性などを表現しやすくなる
- トークンを文産業減として扱える特徴量に変換する操作を**埋め込み**と呼ぶ
- 埋め込みは機械学習では必要不可欠であり、解きたいタスクに適した埋め込み方法を獲得することが重要である
- 埋め込みの獲得を目指すうえで、できるだけ人間による外部知識を使わずにデータから学習することが都合が良い
- 埋め込みのデータによる学習手法としては、手持ちのテキストから問題を作ってそれを解くことで、言語特性を学習する方法が広く用いられる
    1. トークンの穴埋め問題
    2. 次に来るトークン予測
    3. ある文章と次の文章が連続した文章か否かの予測

## 2.2 サブワードの必要性とトークン化

## 2.3 バイト対符号化（Byte Pair Encoding）

## 2.4 サブワードユニグラム言語モデル

## 2.5 SentencePiece

## 2.6 文字単位よりも細かいサブワード分割

## 2.7 トークナイザーは本当に必要なのか

